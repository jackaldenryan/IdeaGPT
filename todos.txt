TODOs:

- for now, just let users use with my API and restrict usage to like $10
- deploy to vercel

- Payment and Free uses
    - Have to be signed in to use at all
    - For each user, we track usage
        - Once it reaches threshold, we prevent further usage for that user
        - It then directs them to a pricing page that shows three options, each one with different usage limits
            - Idea before thinking of economics:
                - Free (20 generations / week)
                - Pro ($2 / month, 200 gens / week)
                    - Cancel easily anytime and retain membership for the remainder of the month
                - Unlimited ($6 / month, 2,000 generations / week)
- Ensure checkout, pricing, settings pages all work properly
- Vercel

- Checks before actually deploy:
    - security
        - SQL injection
        - user input validation
        - account and data security
        - payment information security




UI nice-to-have:
- main app page: prompt box too low
- auto scroll to bottom when new ideas generated
- Sign in-page
    - implement "remember me"
    - implement email verification
    - googel and facebook actually integrate for signing in
    - better font
- implement sign out functionality
- implement user profile management
- clear chat button
    - one day: new chat button + saving old chat
- if there is exact duplicate, checkbox and reason automatically same (is this good to keep? Whats going on?)
- better advanced options menu
- loading symbol
- Demarking line between sets of ideas that had different prompt
    - and maybe some note thing you can hover over that expands and tells you what the prompt was
- "Enter" makes the prompt generate and ctrl-enter adds
- when the text box gets bigger, it exapnds upward and never downward
- Way to select ideas and save them to either a new “idea file” or append them to existing. Idea file is a lot like a previous chat, but you can upload an idea file to a new chat to instantiate it with labelled ideas. Can also download the idea file locally if you want






OLD TODOs

TODOs:
- Does marking good/bad ideas really work?
    - Try adding option to add reason why an idea is good
    - Try with generating interesting sequences
        - Need to allow the user to specify structure of idea, so that can generate code, then also 
    - Try with better model
    - Try fine-tuning
    - Try improving the prompt




LONG TERM GOALS:
- Live website, sign in, pay as you go with slight upcharge from OpenAI API usage
- Ultimate goal: good ideas as quick as possible
    - subgoal: mark good and bad ideas
    - subgoal: generate as many ideas as want (until gen too slow or stops)
    - subgoal: no duplicate ideas
Extra:
- Can specify JSON structure of "idea" eg "description of idea + code of idea"
- Every row is an idea object, and can apply a python script to columns to generate new columns as you go. 
    - For example, inputting the idea code into larger python script, and:
        - outputting whether failed or succeeded on test cases
        - Or outputing the PNG of a graph
        - outputing a png of a website page


Nice to have (later):
- try out all the different avoiding duplicate methods:
    - approx avoid duplicates by including past ideas in prompt
    - definitely avoid duplicates with vector search, temperature, and retrying (+prompt caching with claude)
    - approx avoid duplicates by increasing temp over time at some rate
- avoid duplicates
    - With Claude, can do this cheaply with prompt caching
    - W/o prompt caching:
        - might work to just auto-reprompt with minimal needed context and without previous ideas, but with higher and higher temp
          and if an idea is rejected via vector search we try again. Any downsides to higher temp? If yes, then tradeoff is these downsides vs token cost of including prev ideas in prompt
- ensure the user-entered number of ideas is generated, definitely
    - way to force using response_format?
    - or just use a while loop until right number
- user can specify the structure of an idea
    - eg description + code
- support slight modification to prompt but not losing ideas/good marked ideas
- able to download CSV of ideas and label (good vs not)
- button to clear all good ideas
- button to clear all ideas
- empty prompt is not allowed
- generate small or large variations of specific idea
- automatically send generated code to local computer, run it in a certain place in code base, and send back outputs/artifacts from that run
    - For instance, 
        - automatically get matplotlib graph
        - get results of tests
        - get picture of how web page looks
    - Then, can 
- Option to automatically tie good/bad label to one of the idea's artifacts (eg results of code tests)
    - If this is an option, then also want option to automatically hit "Generate more" button after labelling has been done. Then you can press single button and get good code
- is there a nicer way to prevent user from generating completely different ideas after generating some? it kind of ruins everything
    - Maybe have interface for removing a set of past ideas (deleting them from history)



Notes:
- Potential issues with generating many ideas in one prompt:
    - If ask for too many, model might stop really early, before even runs out of context
    - IF ask for too many, ideas start deteriorating
    - more duplicates if ask for a lot?



Applications;
- startup ideas
- name ideas
- code snippets
- math proof directions
- research ideas
- inventing new math
- social situation handling
- wisdom
- message response ideas (email, dating apps, copy, cover letter, etc)


Thoughts:
- Code complete that auto tests ideas
   - Start by aiming for: make an API that autogenerates until specified stop condition (like passing test, or txt file equals something)
- Idea gen better when can verify if idea good - as opposed to being unable to evaluate an idea
- Idea gen with stop condition could be used to automatically find proofs, eg generating LEAN theorems and checking validity - perhaps just need to spend enough money on trying ideas and ensuring the LLM doesnt duplicate ideas
- Prompt caching means we can ensure every individual idea is not duplicate cheaply
